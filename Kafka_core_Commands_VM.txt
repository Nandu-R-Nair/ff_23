Make sure that all zookeeper instances are stopped.

sudo zookeeper-server-stop.sh $KAFKA_HOME/config/zookeeper.properties
==============================================================================================
zookeeper-server-start.sh $KAFKA_HOME/config/zookeeper.properties > /tmp/zkservinit.log 2>&1 &

kafka-server-start.sh $KAFKA_HOME/config/server.properties > /tmp/kfservinit.log 2>&1 &

kafka-topics.sh --list --zookeeper localhost:2181
kafka-topics.sh --zookeeper localhost:2181 --delete --topic first-topic

kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic first-topic

kafka-topics.sh --list --zookeeper localhost:2181

kafka-topics.sh --zookeeper localhost:2181 --describe --topic first-topic

kafka-console-producer.sh --broker-list localhost:9092 --topic first-topic
>This is a message
>This is another message

kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic first-topic --from-beginning

Consumer Group
--------------
kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 3 --topic first-topic
# Make sure topic is created with 3 partitions

kafka-console-producer.sh --broker-list localhost:9092 --topic first-topic
>This is message 1
>This is message 2
>This is message 3
>This is message 4
>This is message 5
>This is message 6

# Open terminal 2 and run a consumer
kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic first-topic --group my-first-application
# Open terminal 3 and run another consumer
kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic first-topic --group my-first-application
# Open terminal 4 and run another consumer
kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic first-topic --group my-first-application
# Now if we produce messages from producer terminal we will see that the messages are partitioned

# We can kill one of the consumers with Control+C and give more messages on producer
# We will notice that the messages are partitioned between existing consumers

Offset
------
kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic first-topic --group my-second-application --from-beginning
# The above will display all messages from the beginning
# Close this consumer. Run it again
kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic first-topic --group my-second-application --from-beginning
# We will see that beginning messages are not shown because offset is set to last message that was read
# Any new message will come through
# Close this consumer. Produce some messages
# We will now run the consumer
kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic first-topic --group my-second-application
# We are not giving the --from-beginning it will read from the last offset

Consumer Group Command
----------------------
kafka-consumer-groups.sh --bootstrap-server localhost:9092 --list
kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group my-second-application
# While a consumer from this group is not running we will see the message as such.
# When one or two consumers from this group are running then we will be able to watch the consumers
# Lag can be show as well

# Reset the offset
kafka-consumer-groups.sh --bootstrap-server localhost:9092 --group my-second-application --reset-offsets --to-earliest --execute --topic first-topic

# We will now run the consumer
kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic first-topic --group my-second-application --from-beginning
# We will see that beginning messages are not shown because offset is set
# Close this consumer.

# Reset the offset back by 2 messages
kafka-consumer-groups.sh --bootstrap-server localhost:9092 --group my-second-application --reset-offsets --shift-by -2 --execute --topic first-topic

# We will now run the consumer
kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic first-topic --group my-second-application
# We are not giving the --from-beginning it will read from the last offset
# Close this consumer. 

=========================================================
kafka-server-stop.sh $KAFKA_HOME/config/server.properties
zookeeper-server-stop.sh $KAFKA_HOME/config/zookeeper.properties
================================================================

Receiver Based Approach
----------------------------------


Step-0: We may need an initial clean up on VM, as sometimes a zookeeper instance might be running in the background due to some residual effect. So we can stop the unwanted zookeeper service running the following command on a terminal say Terminal #1.
sudo /usr/local/kafka/bin/zookeeper-server-stop.sh
/usr/local/kafka/config/zookeeper.properties

Step-1: We can start a fresh instance of zookeeper and run it in the background with the following command.
 


zookeeper-server-start.sh $KAFKA_HOME/config/zookeeper.properties >
/tmp/zkservinit.log 2>&1 &

Step-2: Now we will start a Kafka server instance and run it in the background with the following command.
kafka-server-start.sh $KAFKA_HOME/config/server.properties >
/tmp/kfservinit.log 2>&1 &

Step-3: The following command lists the existing topics.
kafka-topics.sh --list --zookeeper localhost:2181

Step-4: If we donâ€™t find the topic named kafka-streaming1 in the list we can create it with the command below. kafka-topics.sh --create --zookeeper localhost:2181 --replication- factor 1 --partitions 1 --topic kafka-streaming1

Step-5: Check the if the topic is created or not by listing the topics again with the list command.
kafka-topics.sh --list --zookeeper localhost:2181

Step-6: The command below gives the description of our topic.
kafka-topics.sh --zookeeper localhost:2181 --describe --topic kafka- streaming1

Step-7: We can now start producing messages and send them on to out topic using Kafka console producer using the following command and typing in any test lines of our choice.
kafka-console-producer.sh --broker-list localhost:9092 --topic kafka- streaming1
>spark streaming with kafka streaming data on kafka with spark

Step-8: On another terminal, Terminal #2, using a Kafka console consumer as shown below, we can test if the messages are being sent by the producer to the topic properly. The Kafka console consumer should get the messages typed into the producer in the above step and display them on console.
kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic kafka-streaming1 --from-beginning
This is an optional step. It can be performed as a verification step.

Step-9: On a separate terminal say, Terminal #3, we can run our Kafka-Spark-Streaming application as shown below. The code is in the file named streamingkafka1.py
spark-submit --master local[2] --packages org.apache.spark:spark- streaming-kafka-0-8_2.11:2.0.2 streamingkafka1.py

The option --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.0.2 adds the required libraries and the dependencies while running the application using spark-submit

Now on terminal #1 if we enter new lines as shown in example below, we can see on terminal #3 the words from these messages are counted in real-time and the word count gets displayed.
>test kafka and spark streaming with streaming on kafka and spark

To stop the streaming application we can press Control+C. Also to stop Kafka console consumer running on terminal #2 also we can press Control+C. Similarly to stop Kafka console produce as well we can press Control+C.
 
Step-10: Remember to delete the topic with the command below as a final clean up at the end i.e. after running our Kafka-Spark Streaming application successfully.
kafka-topics.sh --zookeeper localhost:2181 --delete --topic kafka- streaming1








